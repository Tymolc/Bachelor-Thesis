\section{Implementation}
\label{sec:implementation}

Most of the image processing steps were implemented using OpenCV \footnote{\url{http://opencv.org/}}, an open-source C++ computer vision library.\\
Our system was built in a modular way. Each module fulfilled a specific task. The structure can be seen in figure \ref{fig:class_diagram}. The \texttt{Helper} class handles all the communication with the server, in that it sends the data packets, tells the server which hardware is connected to it, receives information about the current session and performs some more helping procedures. Camera handling is done by the \texttt{CameraHelper}.

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{class_diagram}
\caption{Architecture Diagram}
\label{fig:class_diagram}
\end{figure}

\clearpage

The functions that are most important to us are the \texttt{SensorDataCollector}, which is responsible for receiving the cars sensor data via the OBD II dongle and the \texttt{RacingLineDetector}, which records the racing line of the car with the help of our stereo camera.\\
\texttt{FlagDetector} and \texttt{PotholeDetector} are classes that handle the additional features described in Section \ref{sec:intro}.

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{component_diagram}
	\caption{Component Diagram}
	\label{fig:component_diagram}
\end{figure}
\clearpage
The communication to our sensors and the server is depicted in the component diagram (figure \ref{fig:component_diagram}). All the sensor data is received via the cars CAN Bus and transfered to the OBD II dongle via the OBD II interface. The dongle sets up a Bluetooth connection to our Jetson processing unit, where the data is grabbed by our \texttt{SensorDataCollector}. Also the GPS Dongle sends its data to that component via a USB connection.\\
Our \texttt{CameraHelper} receives the video data transmitted by the stereo camera via USB and distributes them to the video analysis modules.\\
Lastly, our \texttt{Helper} class collects all the data we extracted and sends them to the web server.

\subsection{Recording}
We implemented some of the algorithms described in Section \ref{sec:algorithm} in OpenCV and C++, with special focus on the extraction of the racing line from the camera image and the sensor data collection.

\subsubsection{Visual Odometry}
To extract the Visual Odometry data from the video stream, we used a library called libviso2\footnote{\url{http://www.cvlibs.net/software/libviso/}}, which was developed at the Karlsruhe Institute of Technology (KIT). The library needs a left and right hand rectified image from a stereo camera. The ZED camera from Stereolabs we used already, rectifies the images, so we only need to convert the received stereo image into two data buffers.
For that the left and right image is stored in one OpenCV Mat each, converted into grayscale and then transformed into a uchar buffer.
To receive the images from the camera, we created a \texttt{CameraHelper} class, which centrally grabs the frames and distributes them to the different modules, like the \texttt{RacingLineDetector}, \texttt{FlagDetector}, etc.
\clearpage
Now that the prerequisites are met, we can pass the images to our library.
After analyzing the input as described in section \ref{subsec:vo}, the output will be a 4x4-matrix, the so called pose.

\begin{table}[!ht]
 \begin{center}
  \begin{tabular}{c c c c}
   $R_{11}$ & $R_{21}$ & $R_{31}$ & $T_{x}$\\
   $R_{12}$ & $R_{22}$ & $R_{32}$ & $T_{y}$\\
   $R_{13}$ & $R_{23}$ & $R_{33}$ & $T_{z}$\\
   0 & 0 & 0 & 1
  \end{tabular}
 \end{center}
 \caption{Pose as returned by Visual Odometry algorithm}
\end{table}

The pose consists of the XYZ 3x3 rotation matrix as defined by all $R_{nm}$ and the XYZ translation vector $T_{x/y/z}$. 

\begin{lstlisting}
Mat pose; //position relative to last position
Mat motionMat; //position relative to starting point

while(!finished){
	//library processing
	pose = pose * motionMat;

	float diffX = (pose.at<float>(0, 3));
	float diffZ = (pose.at<float>(1, 3));
	Point2d position = Point2d(diffX, diffY);
	//JSON-object creation 
}
\end{lstlisting}

For our purposes the rotation matrix is not that important, because the racing line is only a series of dots. The rotation of the car doesn't influence the result, so it is necessary to extract the translation vector, store it in a vector object and write it into a list. This object contains the summation of all translation vectors, making up the position relative to the starting point.
At this point we need to connect consecutive dots in the list and draw the resulting lines. The result is the recorded racing line.
\clearpage
This racing line can be used on its own, or combined with GPS, as discussed in the Bachelors Thesis "Precise Racing Car Localisation - Improving GPS through Video and Sensors" by Niklas Hoffmann \cite{hoffmann16}.

\subsubsection{Sensordata}
The OBD-II dongle uses a Bluetooth connection to communicate with our analysis software. Via this connection, which is created automatically at startup by the operating system of our processing unit, we can open a serial port to write and read data from.

Once the connection is established, the serial connection has to be prepared for the specific settings of the dongle.
A couple of control commands put it into the state that we can work with: 

\begin{lstlisting}
void SensorDataCollector::initialize(int fd) {
	//fd is the fileDescriptor, in this case it is the OBD II dongle
	waitForAnswer(fd, "ATZ");	//reset device
	waitForAnswer(fd, "ATL1");	//enable linefeed
	waitForAnswer(fd, "ATSP0");	//autodetect protocol
}
\end{lstlisting}

The function \textit{waitForAnswer} writes the request to the dongle and returns the answer to an optional buffer. For the initialization this buffer is irrelevant, but as soon as we want to receive data, it is necessary.
\clearpage
\begin{lstlisting}
void SensorDataCollector::
	waitForAnswer(int fd, char *buf, string request) {
	int nbuf = 0; 	int i = 0;	const int TIMEOUT = 30;
	bool end_of_answer = false;

	request.append("\r"); //tells the dongle where the request ends
	long writtenBytes = write(fd, request.c_str(), strlen(request.c_str()));
	if (writtenBytes < 0) {
		//errorhandling
	}
	while (!end_of_answer && i < TIMEOUT) {
		long n = read(fd, buf + nbuf, sizeof buf); //read one char at a time
		nbuf += n;
		i++;
		end_of_answer = receivedAnswer(buf); //receivedAnswer checks if last character was a >
	}
}
\end{lstlisting}

After determining which PIDs are supported by the car (using PID 0100), we constantly loop through all of them and calculate the corresponding value. Together with the description, this value is then sent to our server as a JSON\footnote{JavaScript Object Notation} object.

\subsection{Comparison}
We compared our racing lines using a Google Maps integration in our JavaScript web page. This section describes how the prerequisites mentioned in Section \ref{sec:context} are calculated and implemented.

\subsubsection{Google Maps}
In order to properly identify important parts of the track, we decided to connect our recorded racing line with Google Maps.
\clearpage
As our recording approaches can not extract any geographic information, we need at least two GPS points as a reference. One GPS point isn't enough, because that wouldn't specify the direction the car traveled. As the unit of the points returned by VO is meters, we can calculate the corresponding geo-coordinates in relation to that reference point. 

For that we need the starting latitude $\phi_1$, the starting longitude $\lambda_1$, the bearing $\theta$, which is the angle between two VO-points plus the displacement bearing set by the two reference GPS-coordinates, and the distance $\delta$ traveled, which is the distance between the two VO-points.

The resulting GPS-coordinates are calculated like this:

\begin{lstlisting}[language=JavaScript]
for (var k = 1; k < voData.length; k++) {
	calculatedGPS[k] = {};

	var d = Math.sqrt(Math.pow((voData[k].x - voData[0].x - xCorrection), 2) + Math.pow((voData[k].y - voData[0].y - yCorrection), 2)) * voScaleFactor / 1000;
	var stdVec = [0, 1];
	var pointVec = [voData[k].x - voData[0].x - xCorrection, voData[k].y - voData[0].y - yCorrection];

	var bearingOfVoPoint = ((stdVec[0]*pointVec[0])+(stdVec[1]*pointVec[1]))/(Math.sqrt(Math.pow(stdVec[0], 2)+Math.pow(stdVec[1], 2))*Math.sqrt(Math.pow(pointVec[0], 2)+Math.pow(pointVec[1], 2)));
	bearingOfVoPoint = Math.acos(bearingOfVoPoint);
	bearingOfVoPoint = (pointVec[0] >= 0) ? bearingOfVoPoint : (2*Math.PI)-bearingOfVoPoint;
	bearingOfVoPoint += rotation;

	var lat2 = Math.asin( Math.sin(lat1)*Math.cos(d/R) + Math.cos(lat1)*Math.sin(d/R)*Math.cos(bearingOfVoPoint) );
	var lng2 = lng1 + Math.atan2(Math.sin(bearingOfVoPoint)*Math.sin(d/R)*Math.cos(lat1), Math.cos(d/R)-Math.sin(lat1)*Math.sin(lat2));
	lat2 = deg(lat2);
	lng2 = deg(lng2);

	calculatedGMapsPoints.push(new google.maps.LatLng(lat2, lng2));
}
\end{lstlisting}
\clearpage
In order to depict the sensor data, we color in every line segment according to a two-color gradient. Because of that, we get a lot of line objects, which make further processing difficult. Our solution was to use an additional hidden polyline, that contains all the line segments. This polyline is wider than the normal lines, to make hovering on it easier and the fact, that it is one single object, improves performance and enables us to calculate the length of the racing line as well.

\clearpage